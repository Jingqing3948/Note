---
title: 技术学习：吴恩达机器学习
date: 2025-05-31
tags: 
- Machine Learning
categories:
- 算道求索（课外IT技能学习）
- Lecture
description: 吴恩达机器学习课程学习笔记
---

## 前言

学习自 Coursera 的吴恩达老师机器学习系列。https://www.coursera.org/specializations/machine-learning-introduction

## 机器学习定义

计算机不经过明确编程的情况下进行学习的研究领域。比如相比直接编程告诉计算机怎么下棋更容易取胜，让计算机自己经过多次尝试总结形成一个具有较高胜率的下棋模型，每一步自行预测下一步棋怎么走让最终胜率更大。

> [!NOTE]
>
> 在 KCL 机器学习笔记中我们学习过：推理（Inference）和学习（Learning）的区别。推理是已知模型和对应的概率（现实生活中很少），而学习是不知道对应的模型和概率需要统计数据训练得到一个近似值。比如投硬币，我们统计了 100 次投硬币概率，30 次正面 70 次反面，那么学习结果就是 0.3 概率正面。

### 监督学习

给定输入 x 和有标签标注的输出 y，如对于天气预报机器学习案例：（昨天有鱼鳞状云，今天下雨）这样一条数据，昨天的云是输入数据 x，今天下雨是输出 y（知道确切的结果），让计算机由输入到输出的映射训练模型。再比如：输入是一栋房子的基本信息（几室几厅，面积……）输出是其价格。

监督学习主要分为两种：

- 回归问题（Regression）：从无限多的数字中预测一个数字，如房价推测。
- 分类问题（Classification）：如判断这张图片里的动物是猫还是狗，明天是否下雨，预测结果有限可确定。比如可能 0 代表照片里动物是猫，1 代表是狗，2 代表是老鼠……

### 无监督学习

没有对应的有标注的输出 y。比如找到一些潜在的分类（把现有用户分为几种，分类之前我们并不知道有哪些最终的用户类，是计算机帮我们划分的）。这叫做聚类（Clustering）。除此之外的应用还有异常情况检测，新闻关键词组合提取等。

## 线性回归模型

> [!NOTE]
>
> 模型，就是机器学习算法所需要的最终产物。通过数据和算法得到。比如 DeepSeek 模型，GPT 模型。使用模型很简单其实，我们不需要关心模型内部原理，就像一个黑盒子，我们只要输入数据进去得到输出数据就好。不过这门课程我们主要学习“训练模型的过程”，了解学习原理。

线性回归模型是一种非常基础的回归模型。比如根据输入数据（房子面积）预测输出数据（房价）公式：y = 20000x+100000（我瞎写的）这就是一个简单的线性回归模型。

### 简化版训练过程

线性回归模型训练过程（无敌简化版）：

1. 首先我们有一个训练数据集，里面只有两条数据（0,100000）和（2,140000）。训练数据集数据总数一般用 m 表示。
2. 线性回归模型我们设置成 `y=wx+b`，w 一般用于表示模型参数，b 表示线性回归模型后面加的常量。
3. 带入数据，得到 w = 20000，b = 100000. 完成训练。得到的模型用 `f_wb` 表示。

### 代价函数

不过，一般真实数据都不是那么有规律的，我们的模型并非可以完美拟合（fit）所有数据点。比如下面这个情况。

<img src="https://raw.githubusercontent.com/Jingqing3948/FigureBed/main/mdImages/202506021403294.png" alt=" "  />

这些红叉数据点就不在一条直线上，我们怎么可能找到一个完美拟合的线性回归模型经过他们全部呢，做不到。只能尽可能找到一个线性回归模型让这些点离我们模型的偏差尽量小一些。

这就引入了一个机器学习重要概念：代价（cost）也就是我们的模型预测值和真实值的偏差。对于单个数据点偏差一般用损失（loss）表示，比如我预测这栋有 100m^2 的房子房价是 100 万元，实际价格是 110 万元。而代价一般是形容模型整体，所有预测数据和所有训练数据的偏差。

一种成本函数计算公式（成本函数有很多很多选择，就像模型一样，我们也可以不选线性回归模型学习问题）：
$$
J(w, b)=\frac{1}{2m}\sum^{m}\_{i = 1}(f\_{w, b}(x^{(i)})-y^{(i)})^2
$$
$y^{(i)}$ 是第 i 个训练数据的输出（110 万真实房价），$f_{w,b}(x^{(i)})$ 是将第 i 个训练数据输入带入我们的模型得到的预测输出（100 万预测房价）。我们的模型预测目标就是找到合适的模型参数 w，b 让代价函数最小化。

>[!NOTE]
>
>这里前面的参数为什么是 1/2m，因为后续我们要求导来找到让代价函数下降最快的方式，求导的时候就会把 2 约掉，计算更简洁。

下图是一个代价函数可视化案例：

<img src="https://raw.githubusercontent.com/Jingqing3948/FigureBed/main/mdImages/202506021412818.png" alt=" " style="zoom: 33%;" />

对于这个例子我们要找到的代价函数最小值点就是中间那个凹进去的地方对吧。

另一种绘制方式是这样的等高线图：

<img src="https://raw.githubusercontent.com/Jingqing3948/FigureBed/main/mdImages/202506021413456.png" alt=" " style="zoom:50%;" />

### 梯度下降

根据当前训练数据，通过求偏导更新 w，b 让代价函数下降最快（Gradient Descent）。

计算公式：
$$
w = w-\alpha \frac{d}{dw}J(w, b)=\frac{1}{m}\sum^{m}\_{i = 1}(f_{w, b}(x^{(i)})-y^{(i)})x^{(i)}\\\\
b = b-\alpha \frac{d}{db}J(w, b)=\frac{1}{m}\sum^{m}\_{i = 1}(f_{w, b}(x^{(i)})-y^{(i)})
$$
α 是学习率，α 越大每一轮学习的时候我们根据上一轮的代价函数所做的调整就越大。

α 太小的话，会导致需要很多轮才能找到一个比较靠近极小值的模型：

<img src="https://raw.githubusercontent.com/Jingqing3948/FigureBed/main/mdImages/202506021419694.png" alt=" " style="zoom:50%;" />

α 太大有可能总是“迈步迈过头”到达不了极小值点。

<img src="https://raw.githubusercontent.com/Jingqing3948/FigureBed/main/mdImages/202506021419405.png" alt=" " style="zoom:50%;" />

另外，一个学习问题可能会有多个局部极小值点。选择不恰当大小的学习率也会导致无法到达最小值。

<img src="https://raw.githubusercontent.com/Jingqing3948/FigureBed/main/mdImages/202506021424024.png" alt=" " style="zoom: 25%;" />

下图是一个梯度下降参数可视化：

<img src="https://raw.githubusercontent.com/Jingqing3948/FigureBed/main/mdImages/202506021425684.png" alt=" " style="zoom:33%;" />

> [!NOTE]
>
> 另外，每轮学习的时候我们可能会只选取部分训练数据进行迭代更新或者全部数据。如果每次都用全部数据计算代价函数，叫做批梯度下降（Batch Gradient Descent BGD）.
>
> 卷积神经网络（CNN）训练不会每次都看整个图像数据，每次只会取出一部分。这样可以加快计算速度而且更不容易出现过拟合。

> [!TIP]
>
> 老师提出的一种寻找学习率的方式：
>
> 如 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, 3, ....
>
> 此外，还可以使用 Adam 算法寻找合适的学习率。原理上和自己试应该差不多，从小到大试，loss 快速下降时停止增加学习率。

### 多维度特征

特征（features）指的就是输入数据多维度的参数，比如：如果只根据房屋面积计算房屋价格，特征就只有一个就是面积。如果根据房屋面积，几室几厅，楼层数，房屋年龄等作为输入数据，就是多特征学习。

我们用 n 表示特征总数，那么输入数据其实就是一个 m*n 的矩阵，m 表示有 m 条数据，n 表示每条数据里面有 n 个特征。$x_j^{(i)}$ 表示第 i 条数据的第 j 个属性。

同时模型参数也会改变，$w=[w_1, w_2, ... w_n]$ 和 $x^{(i)}$ 进行向量乘法得到一个标量数据，再和标量 b 求和得到预测结果。
$$
f_{w, b}(\mathbf{x}) = \mathbf{w}\cdot \mathbf{x} +b = w_1 x_1 + w_2 x_2 + \cdots + w_n x_n + b
$$
在 python 代码实现中，不需要手动循环相乘求和，可以使用 `numpy` 库：

```python
w = np.array([1, 2.5, -3.3])
b = 4
x = np.array([10, 20, 30])

f = np.dot(w,x)+b
```

而且计算机底层会使用带有矢量化的计算机硬件实现，并行加速计算。

同长度向量可以直接做减法。

```python
w = w - 0.1 * b
```

> [!NOTE]
>
> 如果特征量太大，使用正态方程法可以快速求解 w，b。但是这种方法几乎只适用于线性回归学习方法。

> [!NOTE]
>
> 有的时候可能还需要我们自行设计特征，比如根据房子长宽推导房子面积，如果使用二维度线性回归模型效果就不会太好，但是如果将长*宽作为一个新维度特征输入的一维线性回归模型训练就会好一些。

### 特征缩放

多维度特征尽可能采取范围近似的值，让彼此参数差距不大，否则可能导致梯度下降缓慢。

<img src="https://raw.githubusercontent.com/Jingqing3948/FigureBed/main/mdImages/202506021457653.png" alt=" " style="zoom: 33%;" />

简单的特征缩放方法就是（每个数据-平均值）/（最大值-最小值）。但是分母可能是 0，而且对异常值敏感。

z-score normalization 方法是（数据-平均值）/标准差 σ。转换为均值 0，方差 1 的标准正态分布区间。

### 梯度下降收敛

当每轮迭代的梯度下降小于等于收敛值 ε（如 0.001）时，可以称之为收敛。这个时候就可以不用再学习了，再学习的成本与对模型精确度的提升相比不那么合适了。

### 扩展到多项式回归

$$
f_{\vec{w}, b}({x}) = 
{w_1} {x} + 
{w_2} {x^2} + 
{w_3}{x^3} + 
{b}
$$

## 分类

比如：预测一个邮件是否为垃圾邮件，0 false 1 true。

预测方法：比如线性回归模型得到的结果是 0 到 1 范围内，我们将 0-0.5 视作 0,0.5-1 视作 1.

不过相比线性回归模型，下面这个回归模型更适合分类问题：

### 逻辑回归

1. 首先还是得到一个线性回归模型 $z=wx+b$。
2. 然后不直接把这个模型作为预测输出，而是将其传入 sigmoid 函数：$g(z)=\frac{1}{1+e^{-z}}$

为什么用这个函数？因为其范围 0-1，符合我们的分类需求，而且在 0.5 附近斜率比较大，所以更难预测到 0.5 的不确定概率情况。

<img src="https://raw.githubusercontent.com/Jingqing3948/FigureBed/main/mdImages/202506021532641.png" alt=" " style="zoom: 67%;" />

### 逻辑回归的损失函数

不能再使用之前的平方代价函数了，因为转换得到的代价函数并不是一个凸函数（convex，比如 y = x^2 这样的），而可能是一个波浪线状的有多个极小值点的丑陋函数。

所以要使用下面这个损失函数。
$$
L\left( f\_{\vec{w}, b}(\vec{x}^{(i)}),\ y^{(i)} \right) =
\begin{cases}
    -\log\left( f\_{\vec{w}, b}(\vec{x}^{(i)}) \right), & \text{if } y^{(i)} = 1 \\\\
    -\log\left( 1 - f\_{\vec{w}, b}(\vec{x}^{(i)}) \right), & \text{if } y^{(i)} = 0
\end{cases}\\\\
L\left( f\_{\vec{w}, b}(\vec{x}^{(i)}),\ y^{(i)} \right) =
- y^{(i)} \log\left( f\_{\vec{w}, b}(\vec{x}^{(i)}) \right)
- (1 - y^{(i)}) \log\left( 1 - f\_{\vec{w}, b}(\vec{x}^{(i)}) \right)
$$
逻辑回归损失函数图像如下：

<img src="https://raw.githubusercontent.com/Jingqing3948/FigureBed/main/mdImages/202506022137019.png" alt=" " style="zoom: 50%;" />

y = 1 预测值 = 0 的时候导数非常大，y = 0 预测值 = 1 的时候导数非常大，促使预测值向另一端靠近。

相应的代价函数公式：
$$
\begin{aligned}
J(\vec{w}, b) &= \frac{1}{m} \sum_{i = 1}^{m}
L\left( f\_{\vec{w}, b}(\vec{x}^{(i)}),\ y^{(i)} \right)\\\\
&= -\frac{1}{m} \sum_{i = 1}^{m} \left[
y^{(i)} \log\left( f\_{\vec{w}, b}(\vec{x}^{(i)}) \right)
+ (1 - y^{(i)}) \log\left( 1 - f\_{\vec{w}, b}(\vec{x}^{(i)}) \right)
\right]
\end{aligned}
$$
至于梯度下降，公式和之前一样。不过别忘记现在 f 函数的定义不一样了。
$$
w = w-\alpha \frac{d}{dw}J(w, b)=\frac{1}{m}\sum^{m}\_{i = 1}(f_{w, b}(x^{(i)})-y^{(i)})x^{(i)}\\\\
b = b-\alpha \frac{d}{db}J(w, b)=\frac{1}{m}\sum^{m}\_{i = 1}(f_{w, b}(x^{(i)})-y^{(i)})
$$

### 过拟合及解决方法

如下图，左边的模型欠拟合（underfit）因为模型复杂度不够；而右边的模型过拟合（overfit）因为参数太多模型过于复杂了，导致模型能非常好地拟合训练数据，但是对于训练数据之外的数据预测效果不好。

<img src="https://raw.githubusercontent.com/Jingqing3948/FigureBed/main/mdImages/202506022142124.png" alt=" " style="zoom:67%;" />

过拟合的解决方式：

1. 增加训练数据量；
2. 只选择部分对输出数据影响更大的特征作为训练模型的特征，减小模型复杂度；
3. 模型正交化（Regularization），将模型复杂度也列入代价函数公式中，这样模型过于复杂的时候虽然可以很好地拟合训练数据，代价函数仍然很大。比如加上 $\frac{\lambda}{2m}\sum^{n}\_{j=1}w\_j^2$ ，λ 是正交化参数。

$$
J(w, b)=\frac{1}{2m}\sum^{m}\_{i = 1}(f\_{w, b}(x^{(i)})-y^{(i)})^2+\frac{\lambda}{2m}\sum^{n}\_{j = 1}w\_j^2\\\\
w\_j = w\_j - \alpha 
\frac{\lambda}{m} w\_j - \alpha 
\frac{1}{m} \sum_{i = 1}^{m}
\left( f\_{w, b}(\vec{x}^{(i)}) - y^{(i)} \right) x\_j^{(i)}\\\\
b =\frac{1}{m}\sum^{m}\_{i = 1}(f_{w, b}(x^{(i)})-y^{(i)})
$$

对梯度下降的影响其实就是 w 更新的时候前面一项由 wj 变成 (1-αλ/m)wj 了。这个东西 < 1 ，作用是每次迭代都让 wj 尽可能小一些。

## 神经网络

模拟人脑，分为多层对输入数据进行处理，每层有多个神经元，处理完输出给下一层，最后汇总输出。

在更大数据量或者更复杂的问题上表现更好。

<img src="https://raw.githubusercontent.com/Jingqing3948/FigureBed/main/mdImages/202506022157643.png" alt=" " style="zoom: 33%;" />

比如人脸识别学习案例：如下图，第一层神经网络可能着重识别边缘部分，第二层神经网络可能着重识别人脸的某一部分比如眼睛鼻子嘴巴，第三层则是匹配对应的整张人脸。

<img src="https://raw.githubusercontent.com/Jingqing3948/FigureBed/main/mdImages/202506022158185.png" alt=" " style="zoom: 33%;" />

输入数据被称为输入层（input layer），一般不计入总层数。中间的层都叫隐藏层（hidden layer），最后一层叫输出层（output layer）计入总层数，比如上面这个例子是一个 4 层神经网络。

每个神经元的输入 -> 输出处理函数叫做激活函数（activation function），比如我们可能第一层选用线性回归函数，第二层选用 sigmoid 函数……

比如假设第三层是 sigmoid 激活函数，第三层有三个神经元，那么第三层就会输出三个特征，第三层输出向量计算公式为：
$$
a\_1^{[3]}= g(\vec(w)\_1^{[3]}\cdot \vec{a}^{[2]}+b\_1^{[3]})\\\\
a\_2^{[3]}= g(\vec(w)\_2^{[3]}\cdot \vec{a}^{[2]}+b\_2^{[3]})\\\\
a\_3^{[3]}= g(\vec(w)\_3^{[3]}\cdot \vec{a}^{[2]}+b\_3^{[3]})\\\\
$$
这里会涉及到一些矩阵乘法的知识。

例学习问题：输入数据是烘焙咖啡豆温度和时间，输出数据是咖啡豆烘焙结果（刚好或糊了）。

<img src="https://raw.githubusercontent.com/Jingqing3948/FigureBed/main/mdImages/202506022213779.png" alt=" " style="zoom:67%;" />

比如在 TensorFlow 中配置模型的步骤如下：

```python
# 定义模型
model = Sequential(
    [
        tf.keras.Input(shape=(2,)),
        Dense(3, activation='sigmoid', name = 'layer1'),
        Dense(1, activation='sigmoid', name = 'layer2')
     ]
)

# 查看每层参数
W1, b1 = model.get_layer("layer1").get_weights()
W2, b2 = model.get_layer("layer2").get_weights()

# 配置损失函数
model.compile(
    loss = tf.keras.losses.BinaryCrossentropy(),
    optimizer = tf.keras.optimizers.Adam(learning_rate=0.01),
)

# 自动训练
model.fit(
    Xt,Yt,            
    epochs=10, # 用输入数据重复训练10次
)

# 预测
predictions = model.predict(X_testn)
```

使用起来非常简单，调库有啥难的？不过重点还是在于原理理解。

这个例子中，输入数据是 2 维特征的，第一层隐藏层是 3 维的，第二层输出层是 1 维的，我们最终会得到一个预测结果值。

对于第一层的模型参数：

```python
W = np.array([
    [1, -3,  5],
    [2,  4, -6]
])

b = np.array([-1, 1, 2])     # 偏置项，形状为 (3,)
a_in = np.array([-2, 4])     # 输入向量（也记作 a^[0] 或 x），形状为 (2,)
```

这样计算完得到的是一个 1\*3 形状的矩阵，供第二层输入。

在 python 中利用 numpy 做向量矩阵快速乘法的实现公式：

```python
def g(z):
    # 示例激活函数
    return 1 / (1 + np.exp(-z))  # Sigmoid

def dense(A_in, W, B):
    Z = np.matmul(A_in, W) + B
    A_out = g(Z)
    return A_out
```

### ReLU 激活函数

如下图，ReLU 函数公式很简单，输入 <0 时输出=0，输入 > 0 时输出和输入成正比。

![ ](https://raw.githubusercontent.com/Jingqing3948/FigureBed/main/mdImages/202506022219074.png)

sigmoid 函数只有在 0 附近的输入值才能取得比较好的梯度结果，如果输入数据太大了，那么梯度下降特征不明显，可能会造成梯度消失的情况，而且计算复杂；

以及，ReLU 会使一部分神经元的输出为 0，这样就造成了网络的稀疏性，并且减少了参数的相互依存关系，缓解了过拟合问题的发生。

最常见的四种激活函数就是：线性激活函数，sigmoid，ReLU，softmax 后面介绍。

### 激活函数的选择

二元分类问题推荐使用 sigmoid，因为最终推导结果介于 0~1 之间。

回归问题建议使用线性回归激活函数。

回归问题且输出 > 0 推荐使用 ReLU。

比如一个二元神经网络问题，采用的激活函数可能是：前两层是 ReLU，最后一层是 Sigmoid。

此外，隐藏层尽量不要使用线性函数（ReLU 可以用），因为根据公式推导，隐藏层放多个线性回归最后得到的输出也是一个线性回归函数，使得这个神经网络解决不了更加复杂的问题。

### 多分类问题

比如手写数字识别，我们预测的结果介于 0~9 之间。

softmax 常用于解决多分类问题，可以得到多个特征的输出结果。比如对于手写数字识别问题，可能得到的结果是：预测是数字 0 的概率为 0.09,1 的概率是 0.11,2 的概率是……最后选择一个最有可能的结果。

算法公式：
$$
z\_j = \vec{w}\_j \cdot \vec{x} + b\_j, \quad j = 1, \dots, N\\\\
a\_j = \frac{e^{z\_j}}{\sum\limits_{k = 1}^{N} e^{z\_k}} = \mathbb{P}(y = j \mid \vec{x})
$$
<img src="https://raw.githubusercontent.com/Jingqing3948/FigureBed/main/mdImages/202506022229410.png" alt=" " style="zoom: 33%;" />

所有预测概率加起来 = 1.

损失函数：
$$
\text{loss}(a\_1, \dots, a\_N, y) =
\begin{cases}
-\log a\_1 & \text{if } y = 1 \\\\
-\log a\_2 & \text{if } y = 2 \\\\
\quad \vdots & \\\\
-\log a\_N & \text{if } y = N \\\\
\end{cases}
$$




损失函数只有对应真实值的预测概率 = 1 时 = 0，否则预测正确结果的概率越小，损失值越大。

在多分类预测问题中，通常使用 softmax 来作为输出层的激活函数。

### 改进 softmax 函数

根据 python 的一些精度特性，如果不是先计算出概率 a 再带入损失函数，而是直接把 z 带入损失函数计算公式，精度更高。

不要：
$$
\text{Loss} = L(\vec{a}, y) = 
\begin{cases}
  -\log a\_1 & \text{if } y = 1 \\\\
  \vdots \\\\
  -\log a\_{10} & \text{if } y = 10 \\\\
\end{cases}
$$
而是：
$$
L(\vec{z}, y) = 
\begin{cases}
  -\log \left( \frac{e^{z\_1}}{e^{z\_1} + \cdots + e^{z\_{10}}} \right) & \text{if } y = 1 \\\\
  \vdots \\\\
  -\log \left( \frac{e^{z\_{10}}}{e^{z\_1} + \cdots + e^{z\_{10}}} \right) & \text{if } y = 10 \\\\
\end{cases}
$$

在 TensorFlow 中的实现方法应该是：最后一层不再声明为 softmax 层，而是 linear 层，取而代之的是在模型参数设置的时候声明计算逻辑损失。

```python
preferred_model = Sequential(
    [ 
        tf.keras.Input(shape=(2,)),
        Dense(25, activation = 'relu'),
        Dense(15, activation = 'relu'),
        Dense(4, activation = 'linear')   #<-- Note
    ]
)
preferred_model.compile(
    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),  #<-- Note
    optimizer=tf.keras.optimizers.Adam(0.001),
)
```

### 多标签分类问题

比如：判断下面这张图片里面有没有汽车？有没有行人？有没有树？

和多分类问题的区别在于，多分类的结果是互斥的，比如预测出数字应该是 5 就不会是 7 了。

而多标签分类问题的多个结果可以共存，比如既有树，也有人。

多标签分类也会得到多个维度的特征，不过类似 sigmoid 单维度处理方式，每个维度的预测值在 0-1 之间，如果 > 0.5 视作 true。比如对有汽车的概率预测为 0.6，有人的概率预测为 0.7，那么就判断这张图片里有人有车。

### 反向传播

说实话有点太难描述，但是这部分十分重要。读者如果感兴趣一定要去学习一下，其实不难，就是求导反推。

可以快速推导出 w 的变化对最终代价函数 j 的影响。

## 机器学习应用问题

### Debug

如果模型误差很大，下一步怎么做？

1. 获取更多数据。
2. 减少或者增加特征。
3. 增加多项式参数。
4. 调整正则化参数 λ。

### 评估模型

如何评估模型性能？通常我们会将输入数据拿出30%作为测试集，其余70%作为训练集。先用训练集训练，然后用测试集和训练集评估误差。

**注意：测试集和训练集评估误差的时候不要加上正交参数，这个时候最小化参数不是我们想要的结果，我们是在评估模型性能。**

如果模型对训练数据表现良好，对测试数据表现差，说明过拟合。

但其实，**测试集仅仅是我们对模型误差的一种“乐观估计”**。因为测试集通常来自与训练集相似的分布，而且在整个建模过程中，它是提前准备好的。如果我们在调参过程中频繁查看测试集的表现，实际上已经“间接使用”了测试集的信息。这会导致模型过拟合到测试集本身，最终得出的泛化能力评估并不可靠。

取而代之的方案是60%数据作为训练集，20%数据作为交叉验证集（Cross Validation），20%数据作为测试集。对于不同模型（如一次线性，二次线性，三次线性……）我们统一用训练集训练，然后用**交叉验证集**计算代价，注意这里也不要考虑参数正交化项。然后选出交叉验证误差最小的模型。最终我们用测试集评估模型泛化性能。以多项式拟合为例：我们可以用训练集分别训练一次线性、二次多项式、三次多项式等模型，然后在验证集上评估它们的预测误差（此时评估用的 cost 函数不需要包含正则化项），选出在验证集上表现最好的模型。最后，再用测试集来评估这个最终模型在未见数据上的表现。

比如下面这个学习案例：

<img src="https://raw.githubusercontent.com/Jingqing3948/FigureBed/main/mdImages/202506022345189.png" alt=" " style="zoom:67%;" />

degree = 4 的时候训练集和交叉验证集误差都最小，5变化不大，6开始回升了。所以最佳选择4.

分类问题和神经网络学习问题同样。

### Bias and Variance

